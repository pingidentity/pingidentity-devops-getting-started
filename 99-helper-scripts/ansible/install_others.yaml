---
- name: Install Other Components in the Cluster
  hosts: kubernetes_master
  any_errors_fatal: true
  become_user: ubuntu
  vars_files:
    - install_list.yaml

  tasks:
    ####################### Architecture #######
    - name: Gather architecture info
      setup:
        filter: ansible_architecture

    - name: Print system architecture
      debug:
        msg: "System architecture is: {{ ansible_facts['architecture'] }}"

    - name: Set architecture-specific variables
      set_fact:
        arch_suffix: >-
          {{ 'amd64' if ansible_facts['architecture'] == 'x86_64' else 'arm64' }}

    - name: Print Ansible architecture designation
      debug:
        msg: "System architecture is: {{ ansible_facts['architecture'] }}"

    #######################  Helm ##############
    - name: Get helm installation file
      get_url:
        url: "https://get.helm.sh/helm-v{{ helm_version }}-linux-{{ arch_suffix }}.tar.gz"
        dest: "/home/ubuntu/helm-v{{ helm_version }}-linux-{{ arch_suffix }}.tar.gz"
        checksum: "sha256:https://get.helm.sh/helm-v{{ helm_version }}-linux-{{ arch_suffix }}.tar.gz.sha256"
      when: helm == True

    - name: Extract helm
      shell: tar xzvf /home/ubuntu/helm-v{{ helm_version }}-linux-{{ arch_suffix }}.tar.gz && sudo mv linux-{{ arch_suffix }}/helm /usr/local/bin/helm
      args:
        executable: /bin/bash
        creates: /usr/local/bin/helm
      when: helm == True

    - name: Remove helm tarball and extracted folder
      file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /home/ubuntu/linux-{{ arch_suffix }}
        - /home/ubuntu/helm-v{{ helm_version }}-linux-{{ arch_suffix }}.tar.gz
      when: helm == True

    ########################  K9S ##############
    - name: Get K9s installation file
      get_url:
        url: "https://github.com/derailed/k9s/releases/download/v{{ k9s_version }}/k9s_Linux_{{ arch_suffix }}.tar.gz"
        dest: "/home/ubuntu/k9s_Linux_{{ arch_suffix }}.tar.gz"
        checksum: "sha256:https://github.com/derailed/k9s/releases/download/v{{ k9s_version }}/checksums.sha256"
      when: k9s == True

    - name: Extract k9s
      command: sudo tar Cxzvf /usr/local/bin /home/ubuntu/k9s_Linux_{{ arch_suffix }}.tar.gz
      args:
        creates: /usr/local/bin/k9s
      when: k9s == True

    - name: Remove k9s tarball
      file:
        path: /home/ubuntu/k9s_Linux_{{ arch_suffix }}.tar.gz
        state: absent
      when: k9s == True
    #########################  MetalLB  ##############

    - name: Resolve MetalLB version
      set_fact:
        metallb_version_resolved: "{{ metallb_version | default('latest') }}"
      when: metallb == True

    - name: Get latest MetalLB version
      shell: curl -s https://api.github.com/repos/metallb/metallb/releases/latest | grep tag_name | cut -d '"' -f 4 | sed 's/v//'
      args:
        executable: /bin/bash
      register: metallb_latest_version
      when: metallb == True and metallb_version_resolved == "latest"

    - name: Set MetalLB version from latest
      set_fact:
        metallb_version_resolved: "{{ metallb_latest_version.stdout }}"
      when: metallb == True and metallb_version_resolved == "latest"

    - name: Get MetalLB installer
      get_url:
        url: "https://raw.githubusercontent.com/metallb/metallb/v{{ metallb_version_resolved }}/config/manifests/metallb-native.yaml"
        dest: "/home/ubuntu/metallb-native.yaml"
      when: metallb == True

    - name: Apply MetalLB file
      shell: kubectl apply -f metallb-native.yaml && sudo touch /etc/.metalLBInstalled
      args:
        creates: "/etc/.metalLBInstalled"
      when: metallb == True

    - name: Pause for 10 seconds
      pause:
        seconds: 10
      when: metallb == True

    - name: Wait for MetalLB controller and speaker pods to be ready
      shell: kubectl wait --namespace metallb-system --for=condition=ready pod --selector="{{ item }}" --timeout=300s
      loop:
        - app=metallb
        - component=speaker
      when: metallb == True

    - name: Creating MetalLB configuration file
      copy:
        dest: "/home/ubuntu/ipaddress_pool_metal.yaml"
        content: |
          apiVersion: metallb.io/v1beta1
          kind: IPAddressPool
          metadata:
            name: production
            namespace: metallb-system
          spec:
            addresses:
            - 192.168.163.151-192.168.163.250
          ---
          apiVersion: metallb.io/v1beta1
          kind: L2Advertisement
          metadata:
            name: l2-advert
            namespace: metallb-system
      when: metallb == True

    - name: Configure MetalLB
      shell: kubectl apply -f ipaddress_pool_metal.yaml && sudo touch /etc/.metalLBConfigured
      args:
        creates: /etc/.metalLBConfigured
        executable: /bin/bash
      when: metallb == True

    - name: Remove MetalLB installation yaml files
      file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /home/ubuntu/ipaddress_pool_metal.yaml
        - /home/ubuntu/metallb-native.yaml
      when: metallb == True

    ###################  ROOK / CEPH  #################

    - name: Install CertManager prerequisite
      shell: kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v{{ cert_manager_version }}/cert-manager.yaml && sudo touch /etc/.certManagerInstalled
      args:
        creates: /etc/.certManagerInstalled
        executable: /bin/bash
      when: storage == True

    - name: Check if rook directory exists
      stat:
        path: "/home/ubuntu/rook"
      register: rook_dir_stat
      when: storage == True

    # Remove if already there to be sure we get the latest version
    - name: Remove directory
      file:
        path: "/home/ubuntu/rook"
        state: absent
      when: rook_dir_stat.stat.exists and storage == True

    - name: Clone Rook repository
      ansible.builtin.git:
        repo: https://github.com/rook/rook.git
        dest: /home/ubuntu/rook
        single_branch: yes
        version: "v{{ rook_version }}"
      when: storage == True

    - name: Install Rook controller
      shell: cd /home/ubuntu/rook/deploy/examples && kubectl apply -f crds.yaml -f common.yaml -f csi-operator.yaml -f operator.yaml && sudo touch /etc/.rookOperatorInstalled
      args:
        creates: /etc/.rookOperatorInstalled
        executable: /bin/bash
      when: storage == True

    - name: Wait for Rook controller pod to be ready
      command: kubectl wait --namespace rook-ceph --for=condition=ready pod --selector=app=rook-ceph-operator --timeout=120s
      when: storage == True

    - name: Install Rook components
      shell: cd /home/ubuntu/rook/deploy/examples && kubectl apply -f cluster.yaml && sudo touch /etc/.rookClusterInstalled
      args:
        creates: /etc/.rookClusterInstalled
        executable: /bin/bash
      when: storage == True

    # Ugly hack
    - name: Pause for 4 minutes - wait for Rook components to get started
      pause:
        seconds: 240
      when: storage == True

    - name: Check for Rook pods by selector
      command: kubectl get pods --namespace rook-ceph --selector="{{ item }}" -o name
      register: rook_pod_checks
      changed_when: false
      failed_when: false
      loop:
        - app=rook-ceph.cephfs.csi.ceph.com-ctrlplugin
        - app=rook-ceph.cephfs.csi.ceph.com-nodeplugin
        - app=rook-ceph.rbd.csi.ceph.com-ctrlplugin
        - app=rook-ceph.rbd.csi.ceph.com-nodeplugin
        - app=rook-ceph-mgr
        - app=rook-ceph-mon
        - app=rook-ceph-crashcollector
        - app=rook-ceph-osd
      when: storage == True

    - name: Confirm Rook cluster pods are ready (if present)
      command: kubectl wait --namespace rook-ceph --for=condition=ready pod --selector="{{ item.item }}" --timeout=300s
      loop: "{{ rook_pod_checks.results }}"
      when: storage == True and item.stdout != ""

    - name: Creating Block Storage class
      copy:
        dest: "/home/ubuntu/sc-ceph-block.yaml"
        content: |
          apiVersion: ceph.rook.io/v1
          kind: CephBlockPool
          metadata:
            name: replicapool
            namespace: rook-ceph
          spec:
            failureDomain: host
            replicated:
              size: 3
          ---
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
             name: rook-ceph-block
          # Change "rook-ceph" provisioner prefix to match the operator namespace if needed
          provisioner: rook-ceph.rbd.csi.ceph.com
          parameters:
              # clusterID is the namespace where the rook cluster is running
              clusterID: rook-ceph
              # Ceph pool into which the RBD image shall be created
              pool: replicapool

              # (optional) mapOptions is a comma-separated list of map options.
              # For krbd options refer
              # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
              # For nbd options refer
              # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
              # mapOptions: lock_on_read,queue_depth=1024

              # (optional) unmapOptions is a comma-separated list of unmap options.
              # For krbd options refer
              # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
              # For nbd options refer
              # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
              # unmapOptions: force

              # RBD image format. Defaults to "2".
              imageFormat: "2"

              # RBD image features
              # Available for imageFormat: "2". Older releases of CSI RBD
              # support only the `layering` feature. The Linux kernel (KRBD) supports the
              # full complement of features as of 5.4
              # `layering` alone corresponds to Ceph's bitfield value of "2" ;
              # `layering` + `fast-diff` + `object-map` + `deep-flatten` + `exclusive-lock` together
              # correspond to Ceph's OR'd bitfield value of "63". Here we use
              # a symbolic, comma-separated format:
              # For 5.4 or later kernels:
              #imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock
              # For 5.3 or earlier kernels:
              imageFeatures: layering

              # The secrets contain Ceph admin credentials.
              csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
              csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
              csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
              csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

              # Specify the filesystem type of the volume. If not specified, csi-provisioner
              # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
              # in hyperconverged settings where the volume is mounted on the same node as the osds.
              csi.storage.k8s.io/fstype: ext4

          # Delete the rbd volume when a PVC is deleted
          reclaimPolicy: Delete

          # Optional, if you want to add dynamic resize for PVC.
          # For now only ext3, ext4, xfs resize support provided, like in Kubernetes itself.
          allowVolumeExpansion: true
      when: storage == True

    - name: Create block ceph storage class
      shell: kubectl apply -f /home/ubuntu/sc-ceph-block.yaml && sudo touch /etc/.storageClassCreated
      args:
        creates: /etc/.storageClassCreated
        executable: /bin/bash
      when: storage == True

    - name: Creating script to patch storage class (globbing and substitution hack)
      copy:
        dest: "/home/ubuntu/patchsc.yaml"
        content: |
          #!/usr/bin/env bash

          kubectl patch storageclass rook-ceph-block -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

    - name: Set storage class as default
      command: bash /home/ubuntu/patchsc.yaml
      when: storage == True

    - name: Remove Rook installation files
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /home/ubuntu/rook
        - /home/ubuntu/sc-ceph-block.yaml
        - /home/ubuntu/patchsc.yaml
      when: storage == True

    #######################  Ingress  ##############

    - name: Copy Traefik Helm values
      copy:
        src: "{{ playbook_dir }}/../../30-helm/ingress-traefik-values.yaml"
        dest: /home/ubuntu/ingress-traefik-values.yaml
        mode: "0644"
      when: ingress == True

    - name: Set Traefik chart version args
      set_fact:
        traefik_chart_version_arg: "{{ '--version ' ~ traefik_chart_version if (traefik_chart_version | default('') | length > 0) else '' }}"
      when: ingress == True

    - name: Install Traefik Ingress
      shell: helm upgrade --install traefik traefik --repo https://traefik.github.io/charts {{ traefik_chart_version_arg }} --namespace traefik --create-namespace -f /home/ubuntu/ingress-traefik-values.yaml && sudo touch /etc/.traefikInstalled
      args:
        creates: /etc/.traefikInstalled
        executable: /bin/bash
      when: ingress == True

    - name: Set default Traefik ServersTransport settings
      set_fact:
        traefik_serverstransport_name: "{{ traefik_serverstransport_name | default('default-pf-localhost') }}"
        traefik_serverstransport_namespace: "{{ traefik_serverstransport_namespace | default('pinghelm') }}"
      when: ingress == True

    - name: Check for Traefik ServersTransport CRD (traefik.io)
      command: kubectl get crd serverstransports.traefik.io
      register: traefik_crd_io
      failed_when: false
      changed_when: false
      when: ingress == True

    - name: Check for Traefik ServersTransport CRD (traefik.containo.us)
      command: kubectl get crd serverstransports.traefik.containo.us
      register: traefik_crd_containo
      failed_when: false
      changed_when: false
      when: ingress == True

    - name: Set Traefik ServersTransport API group
      set_fact:
        traefik_api_group: "{{ 'traefik.io' if traefik_crd_io.rc == 0 else ('traefik.containo.us' if traefik_crd_containo.rc == 0 else '') }}"
      when: ingress == True

    - name: Fail if Traefik ServersTransport CRD not found
      fail:
        msg: "Traefik ServersTransport CRD not found. Verify Traefik installation and CRDs."
      when: ingress == True and traefik_api_group == ""

    - name: Create Traefik ServersTransport manifest
      copy:
        dest: /home/ubuntu/traefik-serverstransport.yaml
        mode: "0644"
        content: |
          apiVersion: {{ traefik_api_group }}/v1alpha1
          kind: ServersTransport
          metadata:
            name: {{ traefik_serverstransport_name }}
            namespace: {{ traefik_serverstransport_namespace }}
          spec:
            serverName: localhost
            insecureSkipVerify: true
      when: ingress == True

    - name: Ensure Traefik ServersTransport namespace exists
      shell: kubectl create namespace {{ traefik_serverstransport_namespace }} --dry-run=client -o yaml | kubectl apply -f -
      args:
        executable: /bin/bash
      register: traefik_serverstransport_namespace_apply
      changed_when: "'created' in traefik_serverstransport_namespace_apply.stdout or 'configured' in traefik_serverstransport_namespace_apply.stdout"
      when: ingress == True

    - name: Apply Traefik ServersTransport
      command: kubectl apply -f /home/ubuntu/traefik-serverstransport.yaml
      when: ingress == True

    - name: Pause for 5 seconds
      pause:
        seconds: 5
      when: ingress == True

    - name: Confirm ingress controller pod is ready
      command: kubectl wait --namespace traefik --for=condition=ready pod --selector=app.kubernetes.io/name=traefik --timeout=90s
      when: ingress == True

    - name: Get Ingress service components for confirmation
      command: kubectl get svc -n traefik
      register: ingress_status_output
      when: ingress == True

    - name: Ingress controller information
      debug:
        msg: "{{ingress_status_output.stdout_lines}}"
      when: ingress == True

    #######################  Istio  ##############

    - name: Get Istio release bundle
      get_url:
        url: "https://github.com/istio/istio/releases/download/{{ istio_version }}/istio-{{ istio_version }}-linux-{{ arch_suffix }}.tar.gz"
        dest: "/home/ubuntu/istio-{{ istio_version }}-linux-{{ arch_suffix }}.tar.gz"
        checksum: "sha256:https://github.com/istio/istio/releases/download/{{ istio_version }}/istio-{{ istio_version }}-linux-{{ arch_suffix }}.tar.gz.sha256"
      when: istio == True or istioaddons == True

    - name: Extract Istio bundle
      shell: tar xzvf /home/ubuntu/istio-{{ istio_version }}-linux-{{ arch_suffix }}.tar.gz -C /home/ubuntu
      args:
        executable: /bin/bash
        creates: /home/ubuntu/istio-{{ istio_version }}
      when: istio == True or istioaddons == True

    - name: Install istioctl
      command: sudo install -m 0755 /home/ubuntu/istio-{{ istio_version }}/bin/istioctl /usr/local/bin/istioctl
      args:
        creates: /usr/local/bin/istioctl
      when: istio == True

    - name: Install istio
      shell: istioctl install -f /home/ubuntu/istio-{{ istio_version }}/manifests/profiles/demo.yaml --skip-confirmation && sudo touch /etc/.istioInstalled
      args:
        executable: /bin/bash
        creates: /etc/.istioInstalled
      when: istio == True

    - name: Pause for 5 seconds
      pause:
        seconds: 5
      when: istio == True

    - name: Confirm Istio pods are ready
      shell: kubectl wait --namespace istio-system --for=condition=ready pod --selector="{{ item }}" --timeout=90s
      loop:
        - app=istiod
        - app=istio-ingressgateway
        - app=istio-egressgateway
      when: istio == True

    - name: Ensure Istio is installed when enabling add-ons
      command: kubectl get deploy istiod -n istio-system
      register: istiod_check
      changed_when: false
      failed_when: istioaddons == True and istio == False and istiod_check.rc != 0
      when: istioaddons == True

    - name: Install Istio add-ons
      shell: kubectl apply -n istio-system -f /home/ubuntu/istio-{{ istio_version }}/samples/addons && sudo touch /etc/.istioAddonsInstalled
      args:
        executable: /bin/bash
        creates: /etc/.istioAddonsInstalled
      when: istioaddons == True

    - name: Confirm Istio additional pods are ready
      shell: kubectl wait --namespace istio-system --for=condition=ready pod --selector="{{ item }}" --timeout=90s
      loop:
        - app.kubernetes.io/name=grafana
        - app=jaeger
        - app=kiali
        - app.kubernetes.io/name=prometheus
        - app.kubernetes.io/name=loki
      when: istioaddons == True

    - name: Creating patch file for services
      copy:
        dest: "/home/ubuntu/patch-service.yaml"
        content: |
          spec:
            type: LoadBalancer
      when: istioaddons == True and metallb == True

    - name: Patch istio add-on services to use load balancer
      shell: kubectl --namespace istio-system patch service "{{ item }}" --patch-file /home/ubuntu/patch-service.yaml
      loop:
        - grafana
        - kiali
        - tracing
        - prometheus
      when: istioaddons == True and metallb == True

    - name: Remove istio tarball and extracted folder
      file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /home/ubuntu/istio-{{ istio_version }}
        - /home/ubuntu/istio-{{ istio_version }}-linux-{{ arch_suffix }}.tar.gz
        - /home/ubuntu/patch-service.yaml
      when: istio == True or istioaddons == True
