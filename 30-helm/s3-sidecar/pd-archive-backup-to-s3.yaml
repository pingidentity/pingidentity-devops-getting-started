############################################################
# Ping Identity
# This file is an extension of 30-helm/pingdirectory-backup/pingdirectory-periodic-backup.yaml
# The primary changes are the addition of an archive script & the assumption that permissions exist
# for use by the archive script either through environment variables or an IAM role.

############################################################
# Archiving a backup file example to S3
############################################################
# Note that the storage of the backup files in this example
# is shown FOR DEMONSTRATION PURPOSES ONLY.
# Your particular implementation will depend on the desired storage
# and the cloud environment (S3, etc.).
configMaps:
  pingdirectory-fetch:
    data:
      fetch.sh: |-
        #!/bin/sh
        set -e

        # This script assumes it is only being run on a single pod of the topology
        # Demonstration only - no testing or error checking is performed

        # Run the fetch from archive command
        # Assumes credentials are set as environment variables or that the Pod 
        # has an associated IAM role that allows it to write to the S3 bucket
        # userRoot is hardcoded as it is the only backend used in this example
        SERVER_RESTORE_DIR=/tmp/restore
        aws s3 sync s3://<your-s3-bucket-name>/<path>/userRoot/ ${SERVER_RESTORE_DIR}/userRoot/
  pingdirectory-archive:
    data:
      archive.sh: |-
        #!/bin/sh
        set -e

        # This script assumes it is only being run on a single pod of the topology

        # Run the archive command
        # Assumes credentials are set as environment variables or that the Pod 
        # has an associated IAM role that allows it to write to the S3 bucket
        # userRoot is hardcoded as it is the only backend used in this example
        aws s3 sync /opt/pingdirectory-backup s3://<your-s3-bucket-name>/<path>/userRoot/
  pingdirectory-backup:
    data:
      backup.sh: |-
        #!/bin/sh
        set -e

        # This script assumes it is only being run on a single pod of the topology

        # Create a directory for the backup
        mkdir -p /opt/pingdirectory-backup
        # Run the backup and archive the files
        # DEMONSTRATION ONLY! NO ERROR CHECKING OR CONTROLS ARE IN PLACE
        backup --backupDirectory /opt/pingdirectory-backup --backendID userRoot --retainPreviousFullBackupAge '1 day' && /opt/in/archive.sh
  pingdirectory-restore:
    data:
      restore.sh: |-
        #!/bin/sh

        # Read admin password from script argument
        if [ "$#" -ne 1 ]; then
          echo "Expected 1 parameter: admin user password."
          exit 1
        fi
        _adminPassword="${1}"

        # Create a directory for the restore files
        SERVER_RESTORE_DIR=/tmp/restore
        mkdir -p "${SERVER_RESTORE_DIR}"

        # Demonstration of pulling files from S3 bucket with fetch.sh script to $SERVER_RESTORE_DIR
        # Fetch the backup files from storage to the local pod.
        # Note that no error checking is performed in this example.
        /opt/in/fetch.sh

        # Check if replication is enabled
        REPL_ENABLED=false
        dsreplication status --no-prompt --adminPassword "${_adminPassword}" &> /dev/null
        test $? -eq 0 && REPL_ENABLED=true && echo "Replication is enabled and will be paused before the restore"
        test "${REPL_ENABLED}" = "false" && echo "Replication is not enabled"

        pause_replication () {
          if test "${REPL_ENABLED}" = "true"; then
            echo "Executing pre-initialization from within ${SERVER} pod for DN ${USER_BASE_DN}"
            dsreplication pre-external-initialization \
              --retryTimeoutSeconds 60 \
              --baseDN "${USER_BASE_DN}" \
              --no-prompt --ignoreWarnings \
              --adminPassword "${_adminPassword}"
          fi
        }

        resume_replication() {
          if test "${REPL_ENABLED}" = "true"; then
            echo "Initializing replication from within ${SERVER} pod for DN: ${USER_BASE_DN} "
            dsreplication initialize-all \
              --retryTimeoutSeconds 60 \
              --baseDN "${USER_BASE_DN}" \
              --no-prompt --ignoreWarnings \
              --adminPassword "${_adminPassword}"

            echo "Executing post-initialization from within ${SERVER} pod for DN: ${USER_BASE_DN}"
            dsreplication post-external-initialization \
              --retryTimeoutSeconds 60 \
              --baseDN "${USER_BASE_DN}" \
              --no-prompt --ignoreWarnings \
              --adminPassword "${_adminPassword}"
          fi
        }

        # This guarantees that resume_replication is always run, even if the restore job exits due to an error
        trap "resume_replication" EXIT

        pause_replication

        # Print listed files from user data archive
        if ! ls ${SERVER_RESTORE_DIR}; then
          echo "Failed to list ${SERVER_RESTORE_DIR}"
          exit 1
        fi

        if test -f "${SERVER_ROOT_DIR}/changelogDb"; then
          echo "Removing changelogDb before restoring user data"

          if ! rm -rf "${SERVER_ROOT_DIR}/changelogDb"; then
            echo "Failed to remove ${SERVER_RESTORE_DIR}/changelogDb"
            exit 1
          fi
        fi

        echo "Restoring to the latest backups under ${SERVER_RESTORE_DIR}"
        BACKEND_DIRS=$(find "${SERVER_RESTORE_DIR}" -name backup.info -exec dirname {} \;)

        # If encryption-settings backend is present in the backups, it must be restored first.
        # So re-order the backups such that it appears first in the list.
        ORDERED_BACKEND_DIRS=
        ENCRYPTION_DB_BACKEND_DIR=

        for BACKEND_DIR in ${BACKEND_DIRS}; do
          if test "${BACKEND_DIR%encryption-settings}" != "${BACKEND_DIR}"; then
            echo "Found encryption-settings database backend"
            ENCRYPTION_DB_BACKEND_DIR="${BACKEND_DIR}"
          else
            test -z "${ORDERED_BACKEND_DIRS}" &&
                ORDERED_BACKEND_DIRS="${BACKEND_DIR}" ||
                ORDERED_BACKEND_DIRS="${ORDERED_BACKEND_DIRS} ${BACKEND_DIR}"
          fi
        done

        test ! -z "${ENCRYPTION_DB_BACKEND_DIR}" &&
            ORDERED_BACKEND_DIRS="${ENCRYPTION_DB_BACKEND_DIR} ${ORDERED_BACKEND_DIRS}"

        echo "Restore order of backups: ${ORDERED_BACKEND_DIRS}"

        for BACKEND_DIR in ${ORDERED_BACKEND_DIRS}; do
          printf "\n----- Doing a restore from ${BACKEND_DIR} -----\n"
          restore --task \
            --useSSL --trustAll \
            --port ${LDAPS_PORT} \
            --bindDN "${ROOT_USER_DN}" \
            --bindPassword "${_adminPassword}" \
            --backupDirectory "${BACKEND_DIR}" \
            --ignoreCompatibilityWarnings
        done

        # Cleanup
        rm -rf ${SERVER_RESTORE_DIR}

        echo "Restore complete"

pingdirectory:
  # Use a cronJob to run backups every six hours
  cronjob:
    enabled: true
    spec:
      schedule: "0 */6 * * *"
      successfulJobsHistoryLimit: 0
      failedJobsHistoryLimit: 1
    args:
      - /opt/in/backup.sh
  enabled: true
  envs:
    SERVER_PROFILE_URL: https://github.com/pingidentity/pingidentity-server-profiles.git
    SERVER_PROFILE_PATH: getting-started/pingdirectory
  workload:
    # Share process namespace for sidecar to get a view inside the main container
    shareProcessNamespace: true
  # Share /tmp so sidecar can see Java processes. Don't keep /tmp around between restarts though.
  volumes:
    - name: temp
      emptyDir: {}
    - name: archive-script
      configMap:
        name: pingdirectory-archive
        defaultMode: 0755
    - name: backup-script
      configMap:
        name: pingdirectory-backup
        defaultMode: 0755
    - name: fetch-script
      configMap:
        name: pingdirectory-fetch
        defaultMode: 0755
    - name: restore-script
      configMap:
        name: pingdirectory-restore
        defaultMode: 0755
  volumeMounts:
    - name: temp
      mountPath: /tmp

  # Backups, restores, and other CLI tools should be run from the sidecar to prevent interfering
  # with the main PingDirectory container process.
  # The image specification will need modification to point to the image that contains the archive script.
  utilitySidecar:
    enabled: true
    image:
      name: myrepository/sidecar
      tag: "latest"
    volumes:
      - name: archive-script
        mountPath: /opt/in/archive.sh
        subPath: archive.sh
      - name: backup-script
        mountPath: /opt/in/backup.sh
        subPath: backup.sh
      - name: fetch-script
        mountPath: /opt/in/fetch.sh
        subPath: fetch.sh
      - name: restore-script
        mountPath: /opt/in/restore.sh
        subPath: restore.sh
